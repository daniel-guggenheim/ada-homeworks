{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 - Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import nltk\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('hillary-clinton-emails','Emails.csv')\n",
    "df_email_initial = pd.read_csv(filename)\n",
    "df_email_initial.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email = df_email_initial\n",
    "# df_email = df_email_initial[[\"ExtractedSubject\", \"ExtractedBodyText\"]]\n",
    "df_email = df_email.dropna(axis=0, how='any', subset=['ExtractedBodyText'])\n",
    "df_email.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countriesName = list(x.name for x in list(pycountry.countries))\n",
    "results = pd.DataFrame(countriesName, columns=['country'])\n",
    "results['sentiment'] = \"\"\n",
    "results['frequency'] = \"\"\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_countries_abbrev = [\"RE\", \"FM\", \"TV\", \"AL\", \"AQ\", \"LA\", \"BEN\", \"and\", \"is\", 'my', 'no', 'to', 'are', 'in', 'so', 'at',\n",
    "                       'as', 'be', 'by', 'can', 'it', 'am', 'as', 'do', 'us', 'me', 'pm', 'ago', 'arm', 'mn', 'nor', 'lie']\n",
    "bad_countries_abbrev = [abb.lower() for abb in bad_countries_abbrev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatCountry(word):\n",
    "    return (' ' + word + ' ').lower()\n",
    "\n",
    "def makeCountryTable(country):\n",
    "    countryArr = [country.name.lower()]\n",
    "    if(country.alpha_2.lower() not in bad_countries_abbrev):\n",
    "        countryArr.append(formatCountry(country.alpha_2))\n",
    "    if(country.alpha_3.lower() not in bad_countries_abbrev):\n",
    "        countryArr.append(formatCountry(country.alpha_3))\n",
    "    if hasattr(country, 'official_name'):\n",
    "        countryArr.append(country.official_name.lower())\n",
    "    if hasattr(country, 'common_name'):\n",
    "        countryArr.append(country.common_name.lower())\n",
    "    ## Some countries have the form : 'Iran, Islamic Republic of' => we want to \n",
    "    ## to transform it to : 'Iran' and 'Islamic Republic of Iran'\n",
    "    if \",\" in country.name:\n",
    "        split_list = country.name.split(',')\n",
    "        countryArr.append((split_list[1] + ' ' + split_list[0]).lower())\n",
    "        ## There are two entities named Virgin Islands, so we don't take the name alone.\n",
    "        if split_list[0] !=  'Virgin Islands':\n",
    "            countryArr.append((split_list[0]).lower())\n",
    "    ## Special case: We add the name \"Syria\" that is common\n",
    "    if country.name == 'Syrian Arab Republic':\n",
    "        countryArr.append('syria')\n",
    "    \n",
    "    return countryArr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "countryNamesAbbrev = list(makeCountryTable(x) for x in list(pycountry.countries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# attr_x = []\n",
    "# for x in list(pycountry.countries):\n",
    "#     if \",\" in x.name:\n",
    "#         print(x.name)\n",
    "#         ll = x.name.split(',')\n",
    "#         print(ll[1] + ' ' + ll[0])\n",
    "#         if ll[0] !=  'Virgin Islands':\n",
    "#             print(ll[0])\n",
    "# #         print(x)\n",
    "# #         ADD IRAN TO LIST!!!!!! AND CHECK FOR OTHER COUNTRIES\n",
    "# # attr_x\n",
    "# for x in list(pycountry.countries):\n",
    "#     if \"Syria\" in x.name:\n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment score computations: Vader and liu hu lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('opinion_lexicon')\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vader_sentiment_computation(text):\n",
    "    \"\"\"\n",
    "    Function from https://github.com/nltk/nltk/blob/develop/nltk/sentiment/util.py\n",
    "    Output polarity scores for a text using Vader approach.\n",
    "    :param text: a text whose polarity has to be evaluated.\n",
    "    \"\"\"\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    return vader_analyzer.polarity_scores(text).get('compound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computation_liu_hu_lexicon(text):\n",
    "    \"\"\"\n",
    "    Function inspiration from https://github.com/nltk/nltk/blob/develop/nltk/sentiment/util.py\n",
    "    Helper function to compute the number of positive word, of negative words and of neutrals words.\n",
    "    Output: (Number of positive words, number of negative words, number of neutral words)\n",
    "    :param text: a text whose polarity has to be classified.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(text)]\n",
    "#     print(tokenized_sent)\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    neut_words = 0\n",
    "    \n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "#             print('positive: ',word)\n",
    "            pos_words += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "#             print('negative: ', word)\n",
    "            neg_words += 1\n",
    "        else:\n",
    "            neut_words += 1\n",
    "\n",
    "    print('Positive words: ', pos_words)\n",
    "    print('Negative words: ', neg_words)\n",
    "    print('Neutral words: ', neut_words)\n",
    "    return (pos_words, neg_words, neut_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liu_hu_lexicon_absolute(pos_words, neg_words):\n",
    "    \"\"\"\n",
    "    Sentiment classification using Liu and Hu opinion lexicon.\n",
    "    This function compute the liu hu scores by taking the majority voting.\n",
    "    If the majority of the words are positive, the score will be 1, if it\n",
    "    is negative, the score will be -1, otherwise the score will be 0.\n",
    "    :param pos_words: The number of positive words in a text\n",
    "    :param neg_words: The number of negative words in a text\n",
    "    \"\"\"\n",
    "\n",
    "    if pos_words > neg_words:\n",
    "        return 1\n",
    "    elif pos_words < neg_words:\n",
    "        return -1\n",
    "    elif pos_words == neg_words:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liu_hu_lexicon_relative(pos_words, neg_words):\n",
    "    \"\"\"\n",
    "    Sentiment classification using Liu and Hu opinion lexicon.\n",
    "    This function compute the liu hu scores by taking the number of \n",
    "    positive words over the total number of positive and negative words. Therefore it will\n",
    "    ponderate the score with the total number of negative and positive words\n",
    "    and output a continuous score between -1 and 1. -1 beeing when\n",
    "    there is only negative words and 1 when there is only positive words.\n",
    "    :param pos_words: The number of positive words in a text\n",
    "    :param neg_words: The number of negative words in a text\n",
    "    \"\"\"\n",
    "\n",
    "    if pos_words == 0 and neg_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return pos_words / (pos_words + neg_words) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liu_hu_lexicon_neutral_ponderated(pos_words, neg_words, neut_words):\n",
    "    \"\"\"\n",
    "    Sentiment classification using Liu and Hu opinion lexicon.\n",
    "    This function compute the liu hu scores by ponderating the number of positive\n",
    "    words with the total number of positive and negative words. It will then ponderate this \n",
    "    score with the total number of neutral words.\n",
    "    :param pos_words: The number of positive words in a text\n",
    "    :param neg_words: The number of negative words in a text\n",
    "    :param neut_words: The number of neutral words in a text\n",
    "    \"\"\"\n",
    "\n",
    "    if pos_words == 0 and neg_words == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (pos_words / ( neut_words + pos_words + neg_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def execute_liu_hu_lexicon_tests(text):\n",
    "    \"\"\"\n",
    "    Input: A text on which to execute the lexicon task\n",
    "    Output: The results of the three lexicon functions\n",
    "    \"\"\"\n",
    "    (pos_words, neg_words, neut_words) = computation_liu_hu_lexicon(text)\n",
    "    absolute = liu_hu_lexicon_absolute(pos_words, neg_words)\n",
    "    relative = liu_hu_lexicon_relative(pos_words, neg_words)\n",
    "    ponderated = liu_hu_lexicon_neutral_ponderated(pos_words, neg_words, neut_words)\n",
    "    return (absolute, relative, ponderated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to analyze sentiment over the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize dictionnaries to store sentiment\n",
    "vader_sentiment_dict = {}\n",
    "liu_hu_absolute_dict = {}\n",
    "liu_hu_relative_dict = {}\n",
    "liu_hu_ponderated_dict = {}\n",
    "\n",
    "## Initialize a counter to know which country words are used to\n",
    "## find countries, to check for mistakes.\n",
    "country_words_used = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Update a dictionary with the country as key and the result as value.\n",
    "def update_sentiment_dict(country_name, dict_to_update, result):\n",
    "    if(not dict_to_update.get(country_name)):\n",
    "        dict_to_update[country_name] = [result]\n",
    "    else:\n",
    "        dict_to_update[country_name].append(result)\n",
    "\n",
    "## Update the fictionary country_words_used\n",
    "def updateCountryWordUsed(abrev):\n",
    "    if(not country_words_used.get(abrev)):\n",
    "        country_words_used[abrev] = 1\n",
    "    else:\n",
    "        country_words_used[abrev] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def analyzeSentimentInDf(row):\n",
    "    emailContent = str(row.ExtractedBodyText).lower()\n",
    "    foundCountryList = []\n",
    "    \n",
    "    ## Iteration over all countries\n",
    "    for country in countryNamesAbbrev:\n",
    "        foundCountry = False\n",
    "        \n",
    "        ## Iteration over all names for a specific country\n",
    "        for abrev in country:\n",
    "            if not foundCountry and abrev in emailContent:\n",
    "                if len(foundCountryList) == 0:\n",
    "                    print('------------------------------- EMAIL N0: ', row.Id, ' -------------------------------')\n",
    "                    print('--------- COUNTRIES ---------')\n",
    "                foundCountry = True\n",
    "                foundCountryList.append(country[0])\n",
    "                updateCountryWordUsed(abrev)\n",
    "                print(country[0], ': ',abrev)\n",
    "    \n",
    "    ## If we found at least one country name in email, we compute sentiment scores\n",
    "    if len(foundCountryList) != 0:\n",
    "        print('-----------------------------------')\n",
    "        vader_score = vader_sentiment_computation(emailContent)\n",
    "        (abs_lex_score, rel_lex_score, pond_lex_score) = execute_liu_hu_lexicon_tests(emailContent)\n",
    "        \n",
    "        ## Update dictionnaries containing scores:\n",
    "        for f_country in foundCountryList:\n",
    "            update_sentiment_dict(f_country, vader_sentiment_dict, vader_score)\n",
    "            update_sentiment_dict(f_country, liu_hu_absolute_dict, abs_lex_score)\n",
    "#             if rel_lex_score != -1:\n",
    "            update_sentiment_dict(f_country, liu_hu_relative_dict, rel_lex_score)\n",
    "#             if pond_lex_score != -1:\n",
    "            update_sentiment_dict(f_country, liu_hu_ponderated_dict, pond_lex_score)  \n",
    "                \n",
    "        \n",
    "        print('--------- SENTIMENT SCORE ---------')\n",
    "        print('Vader score: ', vader_score)\n",
    "        print('Absolute Lexicon score: ', abs_lex_score)\n",
    "        print('Relative Lexicon score: ', rel_lex_score)\n",
    "        print('Ponderated Lexicon score: ', pond_lex_score)\n",
    "\n",
    "        print('--------- EMAIL CONTENT ---------')\n",
    "        print(emailContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_email = df_email.head(100)\n",
    "sample_email = df_email\n",
    "sample_email.apply(analyzeSentimentInDf, axis=1)\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "keywords_used = sorted(country_words_used.items(), key=operator.itemgetter(1), reverse=True)\n",
    "keywords_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated sentiment tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aggregateSentimentInformation(sentiment_dict):\n",
    "    '''\n",
    "    It will transform the sentiment list in a dictionary and return the same dict\n",
    "    with the mean value for the input list of sentiment value instead of the list.\n",
    "    Input: Dictionary with (country -> List of sentiment values)\n",
    "    Output: Dictionary with (country -> a unique mean value)\n",
    "    '''\n",
    "    resultDict = {}\n",
    "    for country in sentiment_dict:\n",
    "        resultDict[country] = np.mean(sentiment_dict[country])\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeAdvancedDfFromDict(dict_name, sentiment_dict):\n",
    "    '''\n",
    "    Take a dictionnary with (country -> List of sentiment values), ie. {'switzerland': [0.5, 0.1, 1]}\n",
    "    And transform it to a dataframe with as a columm the [country, sentiment, type]\n",
    "    where type is the dict_name. It will make one row per value in the list of sentiment value.\n",
    "    Input: Dictionary with (country -> List of sentiment values)\n",
    "    Output: Dataframe with columns  [country, sentiment, type]\n",
    "    '''\n",
    "    plot_row_lists = []\n",
    "    for country in sentiment_dict:\n",
    "        for val in sentiment_dict[country]:\n",
    "            plot_row_lists.append({'country': country, 'sentiment': val, 'type': dict_name})\n",
    "    return pd.DataFrame(plot_row_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeAdvancedPlotForAllSentimentDict(sentiment_dict_map, y_min= -1.1, y_max = 1.1):\n",
    "    '''\n",
    "    It will take a dictionary of sentiment dictionaries of the form:\n",
    "    (name of the sentiment analysis technique -> sentiment dictionary), where\n",
    "    sentiment dictionary := (country -> List of sentiment values)\n",
    "    Output: Plot with all the dictionaries put together\n",
    "    '''\n",
    "    df_list = []\n",
    "    for dict_name in sentiment_dict_map:\n",
    "        df_list.append(makeAdvancedDfFromDict(dict_name, sentiment_dict_map[dict_name]))\n",
    "    \n",
    "    plot_df = pd.concat(df_list)\n",
    "    \n",
    "    ## Order of columns in plot\n",
    "    order = pd.Series(aggregateSentimentInformation(sentiment_dict_map['vader'])).sort_values().index\n",
    "    \n",
    "    plt.ylim(y_min, y_max)\n",
    "#     plot = sns.stripplot(x=\"country\", y=\"sentiment\", data = plot_df, order = order);\n",
    "    plot = sns.barplot(x=\"country\", y=\"sentiment\", hue=\"type\", data = plot_df, order=order, errwidth=0);\n",
    "    plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_dict_map = {'vader': vader_sentiment_dict,\n",
    "                      'absolute liu hu': liu_hu_absolute_dict, \n",
    "                      'relative liu hu': liu_hu_relative_dict}\n",
    "makeAdvancedPlotForAllSentimentDict(sentiment_dict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makeAdvancedPlotForAllSentimentDict({'vader': vader_sentiment_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "def makePlotForSentimentDict(sentiment_dict, y_min= -1.1, y_max = 1.1):\n",
    "    plot_df = makeAdvancedDfFromDict('analysis', sentiment_dict)\n",
    "    order = pd.Series(aggregateSentimentInformation(sentiment_dict)).sort_values().index # Order of columns in plot\n",
    "    \n",
    "    plt.ylim(y_min, y_max)\n",
    "#     plot = sns.stripplot(x=\"country\", y=\"sentiment\", data = plot_df, order = order);\n",
    "    plot = sns.boxplot(x=\"country\", y=\"sentiment\", data = plot_df, order=order);\n",
    "    plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "    return plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makePlotForSentimentDict(vader_sentiment_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "makePlotForSentimentDict(liu_hu_absolute_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makePlotForSentimentDict(liu_hu_relative_dict, -1.02, 1.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "makePlotForSentimentDict(liu_hu_ponderated_dict, -0.001, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bonus question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import receiver data\n",
    "receiver_filename = os.path.join('hillary-clinton-emails','EmailReceivers.csv')\n",
    "receiverdf = pd.read_csv(receiver_filename)\n",
    "receiverdf.PersonId.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_initial.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "email_senders_map = {}\n",
    "def makeEmailSendersMap(row):\n",
    "    senderId = row.SenderPersonId\n",
    "    if not np.isnan(senderId):\n",
    "        email_senders_map[row.Id] = int(senderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_initial.apply(makeEmailSendersMap, axis=1)\n",
    "email_senders_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Method to create the graph\n",
    "import networkx as nx\n",
    "sendReceiveGraph = nx.Graph()\n",
    "\n",
    "def createGraphWithReceivers(row):\n",
    "    email_id = row.EmailId\n",
    "    receiver_id = row.PersonId\n",
    "    if email_id and receiver_id and email_senders_map.get(email_id):\n",
    "        sendReceiveGraph.add_edge(email_senders_map[email_id], receiver_id, {'email_id':email_id})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "receiverdf.apply(createGraphWithReceivers, axis=1)\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "sorted(sendReceiveGraph.degree_iter(),key=itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = nx.degree(sendReceiveGraph)\n",
    "plt.figure(figsize=(20,15))\n",
    "nx.draw(sendReceiveGraph, nodelist=d.keys(), node_size=[v * 20 for v in d.values()])\n",
    "\n",
    "# plt.draw()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sendReceiveGraph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import community\n",
    "partition = community.best_partition(sendReceiveGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size = float(len(set(partition.values())))\n",
    "pos = nx.spring_layout(sendReceiveGraph)\n",
    "count = 0.\n",
    "for com in set(partition.values()) :\n",
    "    count = count + 1.\n",
    "    list_nodes = [nodes for nodes in partition.keys()\n",
    "                                if partition[nodes] == com]\n",
    "    nx.draw_networkx_nodes(sendReceiveGraph, pos, list_nodes, node_size = 20,\n",
    "                                node_color = str(count / size))\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "nx.draw_networkx_edges(sendReceiveGraph,pos, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(partition.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = {1: {'aa':2, 'bb': 3, 'cc': 1}, 2: {'bb': 5}}\n",
    "aa[1]\n",
    "import heapq\n",
    "heapq.nlargest(3, aa[1], key=aa[1].get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "partitionMailMap = {}  ## map between: {partition id -> { token -> occurence}}\n",
    "\n",
    "for i in range(0,len(set(partition.values()))):\n",
    "    partitionMailMap[i] = {}\n",
    "\n",
    "def addMailWordsToPartition(row):\n",
    "    senderId = row.SenderPersonId\n",
    "    if not np.isnan(senderId):\n",
    "        partitionNum = partition[senderId]\n",
    "        tokenlist = tokenlistmap[row.name] #### ! IN CASE OF tokenlistmap ! \n",
    "        for token in tokenlist:\n",
    "            if partitionMailMap[partitionNum].get(token):\n",
    "                partitionMailMap[partitionNum][token] += 1\n",
    "            else:\n",
    "                partitionMailMap[partitionNum][token] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getMostUsedWordsOfDictionary(token_dict):\n",
    "    '''\n",
    "    Input: Dictionary : {token -> occurence}\n",
    "    Output: List[token_1, token_2, ..., token_20] with occurence(token_i) > occurence(token_j) for i > j\n",
    "    '''\n",
    "    return heapq.nlargest(20, token_dict, key=token_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('We will have', len(set(partition.values())), 'communities.')\n",
    "for partition_id in partitionMailMap:\n",
    "    print('Community', partition_id,':')\n",
    "    most_used_words_list = getMostUsedWordsOfDictionary(partitionMailMap[partition_id])\n",
    "    print(most_used_words_list)\n",
    "#     for word in most_used_words_list:\n",
    "#         print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_email_initial.RawText[0]\n",
    "# stem_tokens[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "G=nx.Graph()\n",
    "# G.add_node(0)\n",
    "# G.add_node(1)\n",
    "G.add_edge('nice', 'hello', emailId=2)\n",
    "G.add_edges_from([(2,3,{'color':'green'}), (3,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pos = nx.spring_layout(G)\n",
    "edge_labels = nx.get_edge_attributes(G,'state')\n",
    "nx.draw(G, pos)\n",
    "nx.draw_networkx_edge_labels(G, pos, labels = edge_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_row_lists = []\n",
    "for country in vader_sentiment_dict:\n",
    "    for val in vader_sentiment_dict[country]:\n",
    "        plot_row_lists.append({'country': country, 'sentiment': val})\n",
    "plot_row_lists[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader_result = aggregateSentimentInformation(vader_sentiment_dict)\n",
    "pd.Series(vader_result).sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vader_result = aggregateSentimentInformation(vader_sentiment_dict)\n",
    "pd.DataFrame.from_dict(vader_result, orient='columns', index='a')\n",
    "results_series = pd.Series(vader_sentiment_dict, name='sentiments_val')\n",
    "results_series.index.name = 'country'\n",
    "plot_df = pd.DataFrame(results_series.reset_index())\n",
    "plot_df.sort_values('sentiments_val', inplace=True)\n",
    "plot_df.head(2)\n",
    "plot_df = pd.DataFrame(plot_row_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "tips = sns.load_dataset(\"tips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot = sns.stripplot(x=\"country\", y=\"sentiment\", data=plot_df);\n",
    "plot.set_xticklabels(plot.get_xticklabels(), rotation=90)\n",
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = ['a', 'b', 'c']\n",
    "b = ['a'] if 'aa' in x else ''\n",
    "[a if a is 'a' else 'b' for a in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk.sentiment.util\n",
    "\n",
    "naive_bayes = NaiveBayesClassifier.train\n",
    "svm = SklearnClassifier(LinearSVC()).train\n",
    "maxent = MaxentClassifier.train\n",
    "\n",
    "nltk.sentiment.util.demo_vader_instance(df_email.ExtractedBodyText[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.sentiment.util.demo_liu_hu_lexicon(df_email.ExtractedBodyText[13], plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_demo_liu_hu_lexicon(sentence, plot=False):\n",
    "    \"\"\"\n",
    "    Basic example of sentiment classification using Liu and Hu opinion lexicon.\n",
    "    This function simply counts the number of positive, negative and neutral words\n",
    "    in the sentence and classifies it depending on which polarity is more represented.\n",
    "    Words that do not appear in the lexicon are considered as neutral.\n",
    "    :param sentence: a sentence whose polarity has to be classified.\n",
    "    :param plot: if True, plot a visual representation of the sentence polarity.\n",
    "    \"\"\"\n",
    "    from nltk.corpus import opinion_lexicon\n",
    "    from nltk.tokenize import treebank\n",
    "\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "    neut_words = 0\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(sentence)]\n",
    "\n",
    "    x = list(range(len(tokenized_sent))) # x axis for the plot\n",
    "    y = []\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            print('positive: ',word)\n",
    "            pos_words += 1\n",
    "            y.append(1) # positive\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            print('negative: ', word)\n",
    "            neg_words += 1\n",
    "            y.append(-1) # negative\n",
    "        else:\n",
    "            neut_words += 1\n",
    "            y.append(0) # neutral\n",
    "    print('Positive words: ', pos_words)\n",
    "    print('Negative words: ', neg_words)\n",
    "    print('Neutral words: ', neut_words)\n",
    "\n",
    "    if pos_words > neg_words:\n",
    "        print('Positive')\n",
    "    elif pos_words < neg_words:\n",
    "        print('Negative')\n",
    "    elif pos_words == neg_words:\n",
    "        print('Neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_demo_liu_hu_lexicon(df_email.ExtractedBodyText[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_email.ExtractedBodyText[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pydic = {'Switzerland': {'CH':[0.2, 0.5, 1.5], 'Suisse': [1,2,3]}, 'Germany':{'GE': [2,2.1, 6, 8], 'Allemagne': [2]} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pydic.get('Switzerland').get('CH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pydic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for a in pydic:\n",
    "    print(pydic.get(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not pydic.get('Switzerlands'):\n",
    "    pydic['Switzerlands'] = [2]\n",
    "pydic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testaaa = {'Switzerland': [0.2, 0.5, 1.5], 'Suisse': [1,2,3], 'Germany': [2,2.1, 6, 8], 'Allemagne': [2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testaaa['aaa'] = [2]\n",
    "testaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "update_sentiment_dict('key', testaaa, 25)\n",
    "testaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(testaaa['Germany'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(2 + 2.1 + 6 + 8)/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyzeSentimentInDfOLD(row):\n",
    "    emailContent = str(row.ExtractedBodyText).lower()\n",
    "#     if('syria' in emailContent):\n",
    "#             if (row.ExtractedSubject):\n",
    "#                 print(\"Subject: \",row.ExtractedSubject, \"\\n\")\n",
    "#             print(emailContent)\n",
    "#             print('-----------------------------------------------------------------------------')\n",
    "    foundCountryGlobal = False\n",
    "    for country in countryNamesAbbrev:\n",
    "        foundCountry = False\n",
    "        for abrev in country:\n",
    "            if not foundCountry and abrev in emailContent:\n",
    "                if(not foundCountryGlobal):\n",
    "                    print('------------------------------- EMAIL N0: ', row.Id, ' -------------------------------')\n",
    "                    print('COUNTRIES:')\n",
    "                foundCountry = True\n",
    "                foundCountryGlobal = True\n",
    "                print(country[0], ': ',abrev)\n",
    "    if foundCountryGlobal:\n",
    "        pass\n",
    "        print('--------- EMAIL CONTENT ---------')\n",
    "\n",
    "        print(emailContent)\n",
    "#         if any(abrev in emailContent for abrev in country):\n",
    "#             print(abrev)\n",
    "#         print(emailContent)\n",
    "#         if('9.11' in emailContent):\n",
    "#             print(emailContent)\n",
    "#             print('-----------------------------------------------------------------------------')\n",
    "#             print('-----------------------------------------------------------------------------')\n",
    "\n",
    "#         if any(abrev in emailContent for abrev in country):\n",
    "#             print('hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ADA]",
   "language": "python",
   "name": "Python [ADA]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
