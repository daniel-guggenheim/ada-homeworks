{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from tqdm import tqdmtopics_data = lda.print_topics(num_topics=5, num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# packages and collections to download\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from csv and preliminary clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>DocNumber</th>\n",
       "      <th>MetadataSubject</th>\n",
       "      <th>MetadataTo</th>\n",
       "      <th>MetadataFrom</th>\n",
       "      <th>SenderPersonId</th>\n",
       "      <th>MetadataDateSent</th>\n",
       "      <th>MetadataDateReleased</th>\n",
       "      <th>MetadataPdfLink</th>\n",
       "      <th>MetadataCaseNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>ExtractedTo</th>\n",
       "      <th>ExtractedFrom</th>\n",
       "      <th>ExtractedCc</th>\n",
       "      <th>ExtractedDateSent</th>\n",
       "      <th>ExtractedCaseNumber</th>\n",
       "      <th>ExtractedDocNumber</th>\n",
       "      <th>ExtractedDateReleased</th>\n",
       "      <th>ExtractedReleaseInPartOrFull</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>RawText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C05739545</td>\n",
       "      <td>WOW</td>\n",
       "      <td>H</td>\n",
       "      <td>Sullivan, Jacob J</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2012-09-12T04:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sullivan, Jacob J &lt;Sullivan11@state.gov&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wednesday, September 12, 2012 10:16 AM</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739545</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN FULL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C05739546</td>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-03-03T05:00:00+00:00</td>\n",
       "      <td>2015-05-22T04:00:00+00:00</td>\n",
       "      <td>DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739546...</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F-2015-04841</td>\n",
       "      <td>C05739546</td>\n",
       "      <td>05/13/2015</td>\n",
       "      <td>RELEASE IN PART</td>\n",
       "      <td>B6\\r\\nThursday, March 3, 2011 9:45 PM\\r\\nH: La...</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  DocNumber                                    MetadataSubject  \\\n",
       "0   1  C05739545                                                WOW   \n",
       "1   2  C05739546  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...   \n",
       "\n",
       "  MetadataTo       MetadataFrom  SenderPersonId           MetadataDateSent  \\\n",
       "0          H  Sullivan, Jacob J            87.0  2012-09-12T04:00:00+00:00   \n",
       "1          H                NaN             NaN  2011-03-03T05:00:00+00:00   \n",
       "\n",
       "        MetadataDateReleased  \\\n",
       "0  2015-05-22T04:00:00+00:00   \n",
       "1  2015-05-22T04:00:00+00:00   \n",
       "\n",
       "                                     MetadataPdfLink MetadataCaseNumber  \\\n",
       "0  DOCUMENTS/HRC_Email_1_296/HRCH2/DOC_0C05739545...       F-2015-04841   \n",
       "1  DOCUMENTS/HRC_Email_1_296/HRCH1/DOC_0C05739546...       F-2015-04841   \n",
       "\n",
       "                         ...                         ExtractedTo  \\\n",
       "0                        ...                                 NaN   \n",
       "1                        ...                                 NaN   \n",
       "\n",
       "                              ExtractedFrom ExtractedCc  \\\n",
       "0  Sullivan, Jacob J <Sullivan11@state.gov>         NaN   \n",
       "1                                       NaN         NaN   \n",
       "\n",
       "                        ExtractedDateSent ExtractedCaseNumber  \\\n",
       "0  Wednesday, September 12, 2012 10:16 AM        F-2015-04841   \n",
       "1                                     NaN        F-2015-04841   \n",
       "\n",
       "  ExtractedDocNumber ExtractedDateReleased ExtractedReleaseInPartOrFull  \\\n",
       "0          C05739545            05/13/2015              RELEASE IN FULL   \n",
       "1          C05739546            05/13/2015              RELEASE IN PART   \n",
       "\n",
       "                                   ExtractedBodyText  \\\n",
       "0                                                NaN   \n",
       "1  B6\\r\\nThursday, March 3, 2011 9:45 PM\\r\\nH: La...   \n",
       "\n",
       "                                             RawText  \n",
       "0  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...  \n",
       "1  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails_file = os.path.join('hillary-clinton-emails', 'Emails.csv')\n",
    "emails_df = pd.read_csv(emails_file)\n",
    "emails_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MetadataSubject</th>\n",
       "      <th>MetadataDocumentClass</th>\n",
       "      <th>ExtractedSubject</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>RawText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOW</td>\n",
       "      <td>HRC_Email_296</td>\n",
       "      <td>FW: Wow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>HRC_Email_296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B6\\r\\nThursday, March 3, 2011 9:45 PM\\r\\nH: La...</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     MetadataSubject MetadataDocumentClass  \\\n",
       "0                                                WOW         HRC_Email_296   \n",
       "1  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...         HRC_Email_296   \n",
       "\n",
       "  ExtractedSubject                                  ExtractedBodyText  \\\n",
       "0          FW: Wow                                                NaN   \n",
       "1              NaN  B6\\r\\nThursday, March 3, 2011 9:45 PM\\r\\nH: La...   \n",
       "\n",
       "                                             RawText  \n",
       "0  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...  \n",
       "1  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...  "
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all redundant information from data frameprint_topics(lda.show_topics(formatted=False, num_words=20))\n",
    "topic_modeling_df = emails_df.copy()\n",
    "topic_modeling_df.drop(['Id'\n",
    "                        , 'DocNumber'\n",
    "                        , 'MetadataTo'\n",
    "                        , 'MetadataFrom'\n",
    "                        , 'SenderPersonId'\n",
    "                        , 'MetadataDateSent'\n",
    "                        , 'MetadataDateReleased'\n",
    "                        , 'MetadataPdfLink'\n",
    "                        , 'MetadataCaseNumber'\n",
    "                        , 'ExtractedTo'\n",
    "                        , 'ExtractedFrom'\n",
    "                        , 'ExtractedCc'\n",
    "                        , 'ExtractedDateSent'\n",
    "                        , 'ExtractedCaseNumber'\n",
    "                        , 'ExtractedDocNumber'\n",
    "                        , 'ExtractedDateReleased'\n",
    "                        , 'ExtractedReleaseInPartOrFull'], axis=1, inplace=True)\n",
    "topic_modeling_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords, msg preprocessing, stemming and tokenizing\n",
    "#### Load list of English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extend set of stopwords modals with modal verbs\n",
    "stopwords = stopwords + ['could', 'may', 'might', 'must', 'ought to', 'shall', 'would']\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def re_longer_than(N):\n",
    "    return re.compile('^[a-z]{' + '{0},'.format(N) + '}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Every message is preprocessed accordng to belonging to english vocabulary and not belonging to stopwords;\n",
    "Also we drop all words with length less than 4 (empirically detected). Of course there are some number of meaningful\n",
    "words, however the majority is considered as a noise.\n",
    "In addition first 6 and last 7 sentences are removed (also empirically detected), because they contain only meta-information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "re_word_filter = re_longer_than(4)\n",
    "\n",
    "def preprocess_msg(msg):\n",
    "    sentences = nltk.sent_tokenize(msg)\n",
    "    del sentences[:6]\n",
    "    del sentences[-7:]\n",
    "    \n",
    "    tokens = []\n",
    "    for s in sentences:\n",
    "        curr_tokens = nltk.word_tokenize(s)\n",
    "        curr_tokens = [word for word in curr_tokens if word in english_vocab]\n",
    "        curr_tokens = [word for word in curr_tokens if word not in stopwords]\n",
    "        curr_tokens = list(filter(lambda x: re_word_filter.match(x) ,curr_tokens))\n",
    "        tokens = tokens + curr_tokens\n",
    "\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Porter Stemmer which is actually part of NLTK. We should do **stemming** - breaking a word down into its root, in order to eliminate an influence of identical words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "# we use stem_map to undo stem and get meaningful word from stem_token. (rough operation)\n",
    "curr_stem_map = {}\n",
    "\n",
    "# as we recalculate stem_map for every group of msgs, we should clean it up\n",
    "def cleanup_stem(): curr_stem_map.clear()\n",
    "\n",
    "# executes stemming and mapping for a token\n",
    "def bind_stem(token):\n",
    "    t_stem = p_stemmer.stem(token)\n",
    "    s = curr_stem_map.get(t_stem, set())\n",
    "    if not s:\n",
    "        curr_stem_map[t_stem] = s\n",
    "        curr_stem_map[t_stem].add(token)\n",
    "    return t_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stems a list of tokens and returns two-dim list of tokens (list per msg)\n",
    "def steamming(tokens_per_msg):\n",
    "    return [[bind_stem(t) for t in tokens] for tokens in tokens_per_msg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function performs:\n",
    "#                    1) preprocessing for every message within a group in order to obtain appropriate tokens\n",
    "#                    2) stemming of tokens\n",
    "#                    3) the Dictionary() function which traverses texts, assigning a unique integer id to each unique \n",
    "#                       token while also collecting word counts and relevant statistics.\n",
    "#                    4) the doc2bow() function which converts dictionary into a bag-of-words. The result, corpus, is a \n",
    "#                       list of vectors equal to the number of documents; doc2bow() only includes terms \n",
    "#                       that actually occur: terms that do not occur in a document will not appear in that document’s \n",
    "#                       vector.\n",
    "#                    5) applying the LDA model\n",
    "\n",
    "# returns calculated lda_model or None in case we have an empty dictionary (possible case)\n",
    "\n",
    "def calculate_topics(message_group, n_topics):\n",
    "    tokens_per_msg = [preprocess_msg(msg) for msg in message_group]\n",
    "    tokens_per_msg = steamming(tokens_per_msg)\n",
    "    dictionary = corpora.Dictionary(tokens_per_msg)\n",
    "    \n",
    "    if not bool(dictionary):\n",
    "        return None\n",
    "    \n",
    "    corpus = [dictionary.doc2bow(tokens) for tokens in tokens_per_msg]\n",
    "    \n",
    "    # tiny optimization, we reguse a number of passes in case group consists of only message\n",
    "    N = 5 if len(message_group) == 1 else 25\n",
    "\n",
    "    return models.ldamodel.LdaModel(corpus, num_topics = n_topics, id2word = dictionary, passes=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a copy of dataframe and prepare column 'MsgTopics' for a topics set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MetadataSubject</th>\n",
       "      <th>MetadataDocumentClass</th>\n",
       "      <th>ExtractedSubject</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>RawText</th>\n",
       "      <th>MsgTopics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOW</td>\n",
       "      <td>HRC_Email_296</td>\n",
       "      <td>FW: Wow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  MetadataSubject MetadataDocumentClass ExtractedSubject ExtractedBodyText  \\\n",
       "0             WOW         HRC_Email_296          FW: Wow               NaN   \n",
       "\n",
       "                                             RawText MsgTopics  \n",
       "0  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...            "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdf_topic_by_group_df = topic_modeling_df.copy()\n",
    "tmdf_topic_by_group_df['MsgTopics'] = \"\"\n",
    "tmdf_topic_by_group_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group messages by subject\n",
    "We want to calculate topics modulation for a chain of messages with the same subject, so as we grouped by 'MetadataSubject'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_msgs_by_subject = tmdf_topic_by_group_df.groupby(['MetadataSubject'])\n",
    "gb = grouped_msgs_by_subject.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# auxiliary function that remaps stemmed tokens back to words;\n",
    "# for simplification we take first corresponding word from pre-mapped stem-words\n",
    "\n",
    "def destem_topics(topics_matrix):\n",
    "    res = '' if not topics_matrix else ' '.join([min(curr_stem_map[w]) for w, p in topics_matrix[0][1]])\n",
    "    cleanup_stem()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run calculation for every group and fill the DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4173/4173 [17:28<00:00,  3.98it/s]\n"
     ]
    }
   ],
   "source": [
    "for key, indices in tqdm(gb.items(), ncols=len(gb)):\n",
    "    group = grouped_msgs_by_subject.get_group(key)\n",
    "    msg_dict = group['RawText'].to_dict()\n",
    "    lda = calculate_topics(list(msg_dict.values()), n_topics=5)\n",
    "    \n",
    "    # get topics from lda model and perform destemming\n",
    "    topics_str = '' if not lda else destem_topics(lda.show_topics(formatted=False, num_words=5))\n",
    "    \n",
    "    # fill 'MsgTopics' column with appropriate topics string\n",
    "    for i in indices:\n",
    "        tmdf_topic_by_group_df.ix[i, 'MsgTopics'] = topics_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MetadataSubject</th>\n",
       "      <th>MetadataDocumentClass</th>\n",
       "      <th>ExtractedSubject</th>\n",
       "      <th>ExtractedBodyText</th>\n",
       "      <th>RawText</th>\n",
       "      <th>MsgTopics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WOW</td>\n",
       "      <td>HRC_Email_296</td>\n",
       "      <td>FW: Wow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "      <td>government also still resent million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...</td>\n",
       "      <td>HRC_Email_296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B6\\r\\nThursday, March 3, 2011 9:45 PM\\r\\nH: La...</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "      <td>military convinced zone support former</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHRIS STEVENS</td>\n",
       "      <td>HRC_Email_296</td>\n",
       "      <td>Re: Chris Stevens</td>\n",
       "      <td>Thx</td>\n",
       "      <td>UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...</td>\n",
       "      <td>corn front sorry devotedly former</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     MetadataSubject MetadataDocumentClass  \\\n",
       "0                                                WOW         HRC_Email_296   \n",
       "1  H: LATEST: HOW SYRIA IS AIDING QADDAFI AND MOR...         HRC_Email_296   \n",
       "2                                      CHRIS STEVENS         HRC_Email_296   \n",
       "\n",
       "    ExtractedSubject                                  ExtractedBodyText  \\\n",
       "0            FW: Wow                                                NaN   \n",
       "1                NaN  B6\\r\\nThursday, March 3, 2011 9:45 PM\\r\\nH: La...   \n",
       "2  Re: Chris Stevens                                                Thx   \n",
       "\n",
       "                                             RawText  \\\n",
       "0  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...   \n",
       "1  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...   \n",
       "2  UNCLASSIFIED\\r\\nU.S. Department of State\\r\\nCa...   \n",
       "\n",
       "                                MsgTopics  \n",
       "0    government also still resent million  \n",
       "1  military convinced zone support former  \n",
       "2       corn front sorry devotedly former  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmdf_topic_by_group_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling over the entire corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to calculate topics for all messages in dataframe. So get 'RawText' message data as a list and apply previously described pipeline: preprocessing -> stemming -> dictionary -> corpus -> lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_list = topic_modeling_df['RawText'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [preprocess_msg(msg) for msg in test_list]\n",
    "tokens = steamming(tokens)\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# auxiliary function; processes topics data obtained from lda model\n",
    "\n",
    "def unstem(topics_data):\n",
    "    topics_data_unstem = []\n",
    "    for topics_item in topics_data:\n",
    "        l = topics_item[1].split(\" + \")\n",
    "        curr_data = []\n",
    "        for i in l:\n",
    "            unstem = min(curr_stem_map[re.search('\"([^\"]*)\"', i).group(1)])\n",
    "            curr_data.append(re.sub('\"([^\"]*)\"', '\\\"{0}\\\"'.format(unstem), i))\n",
    "        topics_data_unstem.append((topics_item[0], '+'.join(curr_data)))\n",
    "    return topics_data_unstem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_topics(topics_matrix):\n",
    "    pure_topics_list = [[min(curr_stem_map[w]) for w, p in words] for i, words in topics_matrix]\n",
    "    for pt in pure_topics_list:\n",
    "        print(pt, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run LDA calculation for [5, 25, 50] topic number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result for topics_number = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_5 = models.ldamodel.LdaModel(corpus, num_topics = 5, id2word = dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.018*\"secure\"+0.017*\"government\"+0.010*\"also\"+0.009*\"military\"+0.008*\"region\"+0.007*\"stated\"+0.007*\"country\"+0.006*\"situation\"+0.006*\"support\"+0.006*\"peace\"'),\n",
       " (1,\n",
       "  '0.020*\"said\"+0.008*\"political\"+0.007*\"president\"+0.006*\"like\"+0.006*\"time\"+0.006*\"deal\"+0.005*\"government\"+0.005*\"administration\"+0.005*\"last\"+0.005*\"party\"'),\n",
       " (2,\n",
       "  '0.010*\"freedom\"+0.010*\"world\"+0.009*\"calling\"+0.008*\"right\"+0.008*\"people\"+0.007*\"peace\"+0.007*\"speech\"+0.007*\"religious\"+0.006*\"human\"+0.006*\"history\"'),\n",
       " (3,\n",
       "  '0.015*\"work\"+0.014*\"know\"+0.014*\"calling\"+0.012*\"time\"+0.009*\"like\"+0.009*\"meeting\"+0.009*\"said\"+0.007*\"also\"+0.007*\"think\"+0.006*\"today\"'),\n",
       " (4,\n",
       "  '0.010*\"work\"+0.009*\"developer\"+0.008*\"needs\"+0.007*\"government\"+0.007*\"publicity\"+0.006*\"people\"+0.006*\"policy\"+0.006*\"economic\"+0.006*\"support\"+0.006*\"also\"')]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_data_lda5 = lda_5.print_topics(num_topics=5, num_words=10)\n",
    "unstem(topics_data_lda5)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secure', 'government', 'also', 'military', 'region', 'stated', 'country', 'situation', 'support', 'peace', 'time', 'source', 'political', 'attack', 'international', 'force', 'concerned', 'well', 'effort', 'work'] \n",
      "\n",
      "['said', 'political', 'president', 'like', 'time', 'deal', 'government', 'administration', 'last', 'party', 'making', 'also', 'former', 'told', 'people', 'even', 'first', 'policy', 'think', 'campaign'] \n",
      "\n",
      "['freedom', 'world', 'calling', 'right', 'people', 'peace', 'speech', 'religious', 'human', 'history', 'opening', 'want', 'text', 'powerful', 'needs', 'like', 'democracy', 'making', 'know', 'last'] \n",
      "\n",
      "['work', 'know', 'calling', 'time', 'like', 'meeting', 'said', 'also', 'think', 'today', 'personal', 'coming', 'morning', 'good', 'people', 'backing', 'week', 'going', 'well', 'talk'] \n",
      "\n",
      "['work', 'developer', 'needs', 'government', 'publicity', 'people', 'policy', 'economic', 'support', 'also', 'world', 'global', 'health', 'diplomacy', 'international', 'million', 'foreign', 'help', 'national', 'building'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_5.show_topics(formatted=False, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result for topics_number = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_25 = models.ldamodel.LdaModel(corpus, num_topics = 25, id2word = dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(21,\n",
       "  '0.019*\"said\"+0.014*\"secure\"+0.009*\"information\"+0.009*\"providing\"+0.009*\"force\"+0.008*\"ordered\"+0.007*\"police\"+0.007*\"also\"+0.007*\"report\"+0.006*\"casing\"'),\n",
       " (8,\n",
       "  '0.019*\"government\"+0.018*\"source\"+0.016*\"also\"+0.014*\"sensitive\"+0.014*\"individual\"+0.012*\"situation\"+0.012*\"secure\"+0.011*\"opinion\"+0.011*\"region\"+0.011*\"stated\"'),\n",
       " (2,\n",
       "  '0.026*\"political\"+0.020*\"conservative\"+0.017*\"company\"+0.016*\"movement\"+0.015*\"group\"+0.015*\"million\"+0.012*\"corporate\"+0.009*\"business\"+0.009*\"liberate\"+0.008*\"like\"'),\n",
       " (6,\n",
       "  '0.024*\"publicity\"+0.020*\"work\"+0.014*\"engagement\"+0.013*\"developer\"+0.011*\"media\"+0.009*\"people\"+0.009*\"health\"+0.009*\"officer\"+0.008*\"technology\"+0.007*\"diplomacy\"'),\n",
       " (13,\n",
       "  '0.022*\"government\"+0.012*\"country\"+0.011*\"military\"+0.009*\"political\"+0.009*\"said\"+0.007*\"report\"+0.006*\"stated\"+0.006*\"time\"+0.006*\"secure\"+0.006*\"local\"')]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_data_lda25 = lda_25.print_topics(num_topics=5, num_words=10)\n",
    "unstem(topics_data_lda25)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['said', 'attack', 'cent', 'secure', 'intelligence', 'official', 'media', 'administration', 'week', 'terrorist', 'campaign', 'last', 'cover', 'told', 'foreign', 'people', 'point', 'source', 'terror', 'meeting'] \n",
      "\n",
      "['freedom', 'history', 'symbolic', 'opening', 'future', 'wall', 'white', 'world', 'people', 'city', 'turn', 'courage', 'century', 'last', 'without', 'alliance', 'built', 'celebration', 'revolution', 'point'] \n",
      "\n",
      "['much', 'think', 'like', 'talk', 'wrote', 'always', 'time', 'good', 'surely', 'know', 'right', 'done', 'work', 'powerful', 'looking', 'party', 'play', 'something', 'backing', 'making'] \n",
      "\n",
      "['government', 'economic', 'percent', 'money', 'needs', 'economy', 'spending', 'like', 'people', 'publicity', 'financial', 'political', 'growth', 'also', 'international', 'first', 'chronic', 'budget', 'debt', 'effect'] \n",
      "\n",
      "['personal', 'registered', 'information', 'confidential', 'press', 'message', 'opinion', 'intended', 'enough', 'best', 'also', 'favor', 'part', 'number', 'course', 'opening', 'book', 'please', 'error', 'unfavorable'] \n",
      "\n",
      "['calling', 'time', 'know', 'meeting', 'tomorrow', 'today', 'morning', 'work', 'like', 'update', 'backing', 'needs', 'want', 'officer', 'confirm', 'tonight', 'week', 'discussion', 'still', 'draft'] \n",
      "\n",
      "['government', 'source', 'also', 'sensitive', 'individual', 'situation', 'secure', 'opinion', 'region', 'stated', 'concerned', 'regime', 'fight', 'former', 'particularly', 'well', 'added', 'believe', 'effort', 'country'] \n",
      "\n",
      "['speech', 'religious', 'freedom', 'like', 'religion', 'human', 'faith', 'information', 'foreign', 'right', 'democracy', 'developer', 'cultural', 'world', 'want', 'condemned', 'political', 'pornographic', 'repressed', 'early'] \n",
      "\n",
      "['said', 'secure', 'information', 'providing', 'force', 'ordered', 'police', 'also', 'report', 'casing', 'made', 'local', 'response', 'military', 'media', 'personnel', 'received', 'considered', 'work', 'federal'] \n",
      "\n",
      "['people', 'work', 'said', 'think', 'know', 'like', 'want', 'going', 'many', 'life', 'world', 'help', 'young', 'coming', 'backing', 'even', 'living', 'every', 'school', 'time'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda_25.show_topics(formatted=False, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Result for topics_number = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = models.ldamodel.LdaModel(corpus, num_topics = 50, id2word = dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28,\n",
       "  '0.028*\"million\"+0.025*\"money\"+0.022*\"company\"+0.015*\"corporate\"+0.013*\"said\"+0.012*\"funds\"+0.012*\"spending\"+0.010*\"business\"+0.008*\"also\"+0.008*\"climate\"'),\n",
       " (33,\n",
       "  '0.033*\"family\"+0.023*\"police\"+0.016*\"said\"+0.014*\"casing\"+0.014*\"home\"+0.012*\"work\"+0.012*\"child\"+0.010*\"legal\"+0.010*\"last\"+0.010*\"take\"'),\n",
       " (46,\n",
       "  '0.164*\"percent\"+0.029*\"disease\"+0.022*\"rating\"+0.018*\"support\"+0.018*\"primary\"+0.016*\"population\"+0.014*\"terror\"+0.014*\"program\"+0.013*\"handling\"+0.013*\"health\"'),\n",
       " (19,\n",
       "  '0.077*\"time\"+0.062*\"route\"+0.031*\"hotel\"+0.027*\"room\"+0.026*\"photo\"+0.024*\"opening\"+0.024*\"delegation\"+0.015*\"regime\"+0.014*\"media\"+0.014*\"staff\"'),\n",
       " (49,\n",
       "  '0.020*\"cent\"+0.019*\"military\"+0.017*\"conflict\"+0.015*\"government\"+0.011*\"police\"+0.010*\"needs\"+0.009*\"major\"+0.009*\"publicity\"+0.008*\"making\"+0.008*\"support\"')]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_data_lda50 = lda.print_topics(num_topics=5, num_words=10)\n",
    "unstem(topics_data_lda50)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['secure', 'attack', 'government', 'violence', 'protect', 'country', 'religious', 'threat', 'also', 'region', 'armed', 'consulate', 'condemned', 'group', 'people', 'violent', 'seen', 'personnel', 'response', 'investigating'] \n",
      "\n",
      "['said', 'director', 'film', 'executive', 'book', 'work', 'festive', 'censorship', 'professional', 'production', 'lead', 'also', 'correct', 'attention', 'political', 'writer', 'many', 'think', 'making', 'action'] \n",
      "\n",
      "['bill', 'vote', 'reform', 'carefully', 'health', 'legislative', 'floor', 'amend', 'debate', 'outright', 'time', 'pass', 'million', 'committee', 'said', 'next', 'major', 'first', 'lunch', 'even'] \n",
      "\n",
      "['former', 'military', 'week', 'think', 'invasion', 'also', 'said', 'inquiry', 'memo', 'like', 'making', 'coming', 'without', 'last', 'take', 'made', 'foreign', 'speech', 'time', 'secretary'] \n",
      "\n",
      "['also', 'death', 'concerned', 'attempt', 'intimidate', 'paper', 'information', 'believe', 'deal', 'coming', 'support', 'dangerous', 'press', 'short', 'attack', 'nationwide', 'intended', 'publisher', 'major', 'reach'] \n",
      "\n",
      "['cent', 'military', 'conflict', 'government', 'police', 'needs', 'major', 'publicity', 'making', 'support', 'army', 'tactic', 'moving', 'command', 'people', 'week', 'number', 'clear', 'debate', 'village'] \n",
      "\n",
      "['needs', 'people', 'work', 'help', 'food', 'local', 'chronic', 'said', 'training', 'school', 'assist', 'many', 'earthquake', 'kids', 'water', 'relief', 'already', 'staff', 'immediately', 'reconstruction'] \n",
      "\n",
      "['information', 'society', 'foreign', 'freely', 'opening', 'civil', 'content', 'site', 'permit', 'region', 'system', 'summit', 'used', 'access', 'religious', 'landed', 'occur', 'effect', 'last', 'accept'] \n",
      "\n",
      "['percent', 'disease', 'rating', 'support', 'primary', 'population', 'terror', 'program', 'handling', 'health', 'nationalist', 'carefully', 'higher', 'congressional', 'port', 'publicity', 'national', 'expressed', 'drive', 'partial'] \n",
      "\n",
      "['party', 'election', 'polling', 'vote', 'fill', 'among', 'position', 'approval', 'favor', 'lead', 'enough', 'opinion', 'major', 'marginalize', 'press', 'liberate', 'like', 'unfavorable', 'conservative', 'today'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_topics(lda.show_topics(formatted=False, num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We can say that after processing topics modulation on corpus we obtained reasonable results of lda. However, we noticed that thematically tokens are more close for topics number = 50. Concerning messages groupped by subject, we see that, the larger group is, the more related tokens are. Since the corpus of groupped messages is not large, a high value of of topics number is meaningless."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
