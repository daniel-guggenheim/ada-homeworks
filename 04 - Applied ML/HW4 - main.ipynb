{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Homework 04 - Applied ML</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data','CrowdstormingDataJuly1st.csv') \n",
    "df = pd.read_csv(filename)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of dyads (rows in dataframe): ', len(df))\n",
    "print('Total number of interactions between a referee and a player (nb of games): ', sum(df.games))\n",
    "print('Mean number of games for a dyad: ', np.mean(df['games']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows in dataframe: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing raters</b>: We decided to remove row where the two rates were significantly different or if any of the rates were absent (Nan value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf = df.copy()\n",
    "\n",
    "## Removing null values in raters\n",
    "cleandf = cleandf[cleandf[\"rater1\"].notnull() & cleandf[\"rater2\"].notnull()]\n",
    "\n",
    "## Removing all rows where the difference between the two raters is larger than 0.25\n",
    "cleandf['difference'] = abs(cleandf.rater1 - cleandf.rater2)\n",
    "cleandf = cleandf[cleandf['difference'] <= 0.25]\n",
    "cleandf.drop('difference', axis =1, inplace=True)\n",
    "\n",
    "print(\"Number of rows in the cleaned dataframe: \", len(cleandf))\n",
    "print(\"Number of rows removed: \", (len(df)-len(cleandf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Skin tone</b>: Then we decide to take the skin tone as the mean between the two raters. This is the value that will be predicted later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleandf[\"meanSkinTone\"] = abs(cleandf[\"rater1\"] + cleandf[\"rater2\"] ) / 2\n",
    "cleandf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Birthday date</b>: As the classifier can not understand date, we decided to change birthday date in seconds. It seemed important for us to keep the birthday date, as it could help predict the color skin if there were more people from a certain demography that played during some years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_to_seconds(t):\n",
    "    seconds = (pd.to_datetime(t) - datetime.datetime(1970, 1,1)).total_seconds()\n",
    "    return int(seconds)\n",
    "\n",
    "cleandf.birthday = cleandf.birthday.apply(time_to_seconds)\n",
    "cleandf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Changing data attributes to numerals function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Dummy variables</i></b>: We noticed that a lot of the columns could not be used in the Random forest as they are non-numerical. As most of these features can be seen as categorical variables, we decided to make dummy variables with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The number of different positions is\", cleandf[\"position\"].unique().size)\n",
    "print(\"The number of different clubs is\",cleandf[\"club\"].unique().size)\n",
    "print(\"The number of different league countries is\",cleandf[\"leagueCountry\"].unique().size)\n",
    "print(\"The number of different referee countries is\",cleandf[\"Alpha_3\"].unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these datas, we decided to remove the \"referee country\" (Alpha_3) and to make dummy variables with the 3 other categories. We decided to remove the referee country because there were a lot of them and it seemed it would induce more error and overfitting to our classifier than it would help it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Replacing Nan</i></b>: We also realize that the dataframe still have some NaN values. We decided to substitute every NaN with the mean of their column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeDfAttributesToNumerals(cleandf):\n",
    "    ## Making the dummy variables\n",
    "    dummydf = pd.get_dummies(cleandf, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"position\"], sparse=False, drop_first=False)\n",
    "    dummydf = pd.get_dummies(dummydf, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"club\"], sparse=False, drop_first=False)\n",
    "    dummydf = pd.get_dummies(dummydf, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"leagueCountry\"], sparse=False, drop_first=False)\n",
    "    dummydf.drop(\"Alpha_3\", axis = 1, inplace = True)\n",
    "    \n",
    "    ## Replacing NaN values\n",
    "    # For each column, if there is any NaN value, we compute the mean and replace the NaN values with it.\n",
    "    for i in range(len(dummydf.columns)):\n",
    "        if (dummydf[dummydf.columns[i]].isnull().values.any()):\n",
    "            mean = np.mean(dummydf[dummydf.columns[i]])\n",
    "            dummydf[dummydf.columns[i]].fillna(mean, inplace = True)\n",
    "    \n",
    "    return dummydf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now apply the function to our dataset and observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummydf = changeDfAttributesToNumerals(cleandf)\n",
    "print(\"Number of columns in the previous dataframe: \",cleandf.columns.size)\n",
    "print(\"Number of columns in the new dataframe with dummy variables: \",dummydf.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummydf.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing useless columns</b>: For the classifier, there are some columns that it makes no sense to use. These columns are the player name (and short name), the photo and the initial ratings (of rater 1 and 2). Therefore we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usedf = dummydf.drop(['playerShort', 'player', 'photoID', 'rater1', 'rater2']  , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "usedf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show how we classify the data using random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def executingRandomForest(X, y, Xpd, printingInfo, numb_trees, nb_features):\n",
    "    # Creating kfolds\n",
    "    once = True\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    sumAccurancy_tr = 0\n",
    "    sumAccurancy_te = 0\n",
    "\n",
    "    #Iterating for kfold validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if(once):\n",
    "            print(\"Train/Test sample sizes:\", train_index.size, \" / \", test_index.size)\n",
    "\n",
    "        ## Making indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #Make classifier\n",
    "        rf = RandomForestClassifier(n_estimators = numb_trees, max_features = nb_features, n_jobs = 4)\n",
    "\n",
    "        #Train the classifier\n",
    "        clf = rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make prediction for testing data with the classifier\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        #Computing error\n",
    "        rf_train_score = metrics.accuracy_score(y_train, clf.predict(X_train))\n",
    "        rf_test_score = metrics.accuracy_score(y_test, y_pred)\n",
    "        if(printingInfo):\n",
    "            print('Train/Test:  {:.4f} / {:.4f}'.format(rf_train_score, rf_test_score))\n",
    "#             print('   {:.4f}'.format(rf_test_score))\n",
    "        \n",
    "        sumAccurancy_tr = sumAccurancy_tr + rf_train_score\n",
    "        sumAccurancy_te = sumAccurancy_te + rf_test_score\n",
    "\n",
    "\n",
    "        # Features importance score\n",
    "        if(once):\n",
    "            once = False\n",
    "            importances = rf.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "                         axis=0)\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    #Printing the final result score:\n",
    "    print('=> Final Train/Test:  {:.4f} / {:.4f}'.format(sumAccurancy_tr / 5.0, sumAccurancy_te / 5.0))\n",
    "    \n",
    "    if(printingInfo):\n",
    "        # Plot the feature importances of the forest\n",
    "        fig = plt.figure()\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(X.shape[1]), importances[indices],\n",
    "                   color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        plt.xticks(range(X.shape[1]), importances[indices])\n",
    "        plt.xlim([-1, X.shape[1]])\n",
    "        plt.show()\n",
    "\n",
    "        # Print the feature ranking\n",
    "        print(\"Feature ranking (first 20):\")\n",
    "        for f in range(20):\n",
    "            print(\"%d. feature %s (%f)\" % (f + 1, Xpd.columns[f], importances[indices[f]]))\n",
    "        \n",
    "    # Returns the average accurancy for the testing set and the training set\n",
    "    return (sumAccurancy_tr / 5.0, sumAccurancy_te / 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making X (the features to use in the classifier) and Y (the value to predict). Executing the random forest algorithm on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMatrixForClassifier(df):\n",
    "    y = np.asarray(df[\"meanSkinTone\"].values, dtype=\"|S6\")\n",
    "    Xpd = df.drop(\"meanSkinTone\", axis = 1)\n",
    "    X = Xpd.as_matrix()\n",
    "    return (X, y, Xpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numb_trees = 10\n",
    "nb_features = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X, y, Xpd) = makeMatrixForClassifier(usedf)\n",
    "executingRandomForest(X, y, Xpd, True, numb_trees, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we have unrealistic results. Ploting the features importance show...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betterResultMaybedf = usedf.drop(['birthday', 'height', 'weight']  , axis=1)\n",
    "(X, y, Xpd) = makeMatrixForClassifier(betterResultMaybedf)\n",
    "executingRandomForest(X, y, Xpd, True, numb_trees, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see that it leads still to a problem. It is in fact the case that we use the same player in both dataset. Therefore the random forest will train by identifying the player, and as he will always have the same skin color, it will help to predict that. We want to avoid that, therefore we will need to group the players together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aggregate the referee info by socker player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to aggregate the referee info, we preprocessed the data in the following way:\n",
    "\n",
    "<ul>\n",
    "<li> One occurrence data - the data that doesn't change (constants): <b>const_columns</b></li>\n",
    "<li> Accumulated data - e.g. victories, yellowCards, etc. :\n",
    "<b>acc_columns</b></li>\n",
    "<li> Majority voting - most frequent data:\n",
    "<b>majority_vote</b></li>\n",
    "<li> Removed data - insignificant columns:\n",
    "<b>remove_columns</b></li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_columns = ['games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']\n",
    "const_columns = ['playerShort', 'player', 'birthday', 'height', 'weight', 'meanSkinTone']\n",
    "majority_vote = ['club', 'leagueCountry', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we remove rater1 and rater2 because we already have calculated 'meanSkinTone' as the mean between those two raters\n",
    "remove_columns = ['photoID', 'refNum', 'refCountry', 'Alpha_3', 'rater1', 'rater2']\n",
    "referee_info_df = cleandf.drop(remove_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After cleaning up, we group the data by player short name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_group_player = list(referee_info_df.groupby('playerShort'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# auxiliary function used in order to accumulate values for columns in 'acc_columns'\n",
    "\n",
    "sum_func = lambda x, y: x+y\n",
    "def accumulate(series):\n",
    "    return reduce(sum_func, series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct aggregation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Having various means, we want to combine - so we average them via calculating weighted mean \n",
    "# (the weight is the sample size)\n",
    "\n",
    "def get_weighted_mean(data):\n",
    "    \n",
    "    # weighted mean calculation for meanIAT\n",
    "    acc_niat = accumulate(data['nIAT'].tolist())\n",
    "    acc_prod_iat = accumulate((data['meanIAT']*data['nIAT']).tolist())\n",
    "    \n",
    "    # weighted mean calculation for meanExp\n",
    "    acc_nexp = accumulate(data['nExp'].tolist())\n",
    "    acc_prod_exp = accumulate((data['meanExp']*data['nExp']).tolist())\n",
    "    \n",
    "    # squareroot of weighted mean of the square for seIAT\n",
    "    acc_se_iat =accumulate((data['nIAT']).tolist())\n",
    "    acc_prod_se_iat = accumulate((data['seIAT']*data['seIAT']*data['nIAT']).tolist())\n",
    "    \n",
    "    # squareroot of weighted mean of the square for seExp\n",
    "    acc_se_exp =accumulate((data['nExp']).tolist())\n",
    "    acc_prod_se_exp = accumulate((data['seExp']*data['seExp']*data['nExp']).tolist())\n",
    "    \n",
    "    return {'weighted_mean_iat' : acc_prod_iat/acc_niat,\n",
    "              'weighted_mean_exp' : acc_prod_exp/acc_nexp,\n",
    "              'sqrt_weighted_mean_iat' : math.sqrt(acc_prod_se_iat/acc_se_iat),\n",
    "              'sqrt_weighted_mean_exp' : math.sqrt(acc_prod_se_iat/acc_se_iat)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we aggregate data in the format of 'list[dict()]' in order to create DataFrame\n",
    "unique_player_data = [] \n",
    "\n",
    "# iterating over the grouped by plater data we assemble all processed values into the dictionary - a row within\n",
    "# an out DataFrame\n",
    "for player_name, data in by_group_player:\n",
    "    \n",
    "    # constants - we keep just first value from every column\n",
    "    one_occurrence = { column : data[column].tolist()[0] for column in const_columns }\n",
    "    \n",
    "    #accumulated values\n",
    "    accumulated = {column : accumulate(data[column].tolist()) for column in acc_columns}\n",
    "    \n",
    "    # majority voting = most_common() gives descending ordered by count, list of pairs;\n",
    "    # most_common()[0][0] - first 0 stands for max voted value ('name': count)\n",
    "    #                       second 0 gives it's name - first element from tuple \n",
    "    majority_vote = { column : Counter(data[column].tolist()).most_common()[0][0] for column in majority_vote}\n",
    "\n",
    "    # weighted mean calculation\n",
    "    wm = get_weighted_mean(data)\n",
    "    \n",
    "    # assemble just calculated data into one dictionary\n",
    "    unique_player_data.append(\n",
    "        dict(list(one_occurrence.items()) +\n",
    "             list(accumulated.items()) +\n",
    "             list(majority_vote.items()) +\n",
    "             list(wm.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create DataFrame from aggregated data\n",
    "aggregated_df = pd.DataFrame(unique_player_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated_df = pd.get_dummies(aggregated_df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"position\"], sparse=False, drop_first=False)\n",
    "aggregated_df = pd.get_dummies(aggregated_df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"club\"], sparse=False, drop_first=False)\n",
    "aggregated_df = pd.get_dummies(aggregated_df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"leagueCountry\"], sparse=False, drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['birthday', 'defeats', 'games', 'goals', 'height', 'meanSkinTone',\n",
       "       'redCards', 'sqrt_weighted_mean_exp', 'sqrt_weighted_mean_iat', 'ties',\n",
       "       ...\n",
       "       'club_Werder Bremen', 'club_West Bromwich Albion',\n",
       "       'club_West Ham United', 'club_Wigan Athletic',\n",
       "       'club_Wolverhampton Wanderers', 'club_Ã‰vian Thonon Gaillard',\n",
       "       'leagueCountry_England', 'leagueCountry_France',\n",
       "       'leagueCountry_Germany', 'leagueCountry_Spain'],\n",
       "      dtype='object', length=129)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregated_df.isnull().values.any()\n",
    "randomForestDF = aggregated_df.drop(['player', 'playerShort'], axis = 1)\n",
    "randomForestDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each column, if there is any NaN value, we compute the mean and replace the NaN values with it.\n",
    "for i in range(len(randomForestDF.columns)):\n",
    "    if (randomForestDF[randomForestDF.columns[i]].isnull().values.any()):\n",
    "        mean = np.mean(randomForestDF[randomForestDF.columns[i]])\n",
    "        randomForestDF[randomForestDF.columns[i]].fillna(mean, inplace = True)\n",
    "        \n",
    "randomForestDF.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9730 / 0.7442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.97299438536379179, 0.74415605159126297)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X, y, Xpd) = makeMatrixForClassifier(randomForestDF)\n",
    "executingRandomForest(X, y, Xpd, False, numb_trees, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_features = len(randomForestDF.columns)\n",
    "nb_trees = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9727 / 0.7378\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9919 / 0.7549\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9986 / 0.7555\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9992 / 0.7524\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9998 / 0.7568\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9997 / 0.7593\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9998 / 0.7549\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7587\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7581\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9998 / 0.7574\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7574\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7580\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7568\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7574\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7593\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7587\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7606\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  1.0000 / 0.7593\n",
      "Train/Test sample sizes: 1266  /  317\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-4c3be3be4972>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_trees\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecutingRandomForest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXpd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"auto\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-114-8ffc93b5162c>\u001b[0m in \u001b[0;36mexecutingRandomForest\u001b[1;34m(X, y, Xpd, printingInfo, numb_trees, nb_features)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#Train the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;31m# Make prediction for testing data with the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sergii\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_more_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m                 tree = self._make_estimator(append=False,\n\u001b[1;32m--> 314\u001b[1;33m                                             random_state=random_state)\n\u001b[0m\u001b[0;32m    315\u001b[0m                 \u001b[0mtrees\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sergii\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0msub\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \"\"\"\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         estimator.set_params(**dict((p, getattr(self, p))\n\u001b[0;32m    121\u001b[0m                                     for p in self.estimator_params))\n",
      "\u001b[1;32mC:\\Users\\Sergii\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mnew_object\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0mparams_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# quick sanity check of the parameters of the clone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sergii\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mget_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                     \u001b[1;31m# if the parameter is deprecated, don't show it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Sergii\\Anaconda3\\lib\\warnings.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, *exc_info)\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot exit %r without entering first\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filters_mutated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowwarning\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showwarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Very long to run, BE CAREFUL\n",
    "training = []\n",
    "testing = []\n",
    "\n",
    "for i in range(10, nb_trees, 10):\n",
    "    tr, te = executingRandomForest(X, y, Xpd, False, i, \"auto\")\n",
    "    training.append(tr)\n",
    "    testing.append(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9725 / 0.7486\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9722 / 0.7397\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9641 / 0.7423\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9692 / 0.7473\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9722 / 0.7341\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9725 / 0.7340\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9692 / 0.7328\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9747 / 0.7341\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9692 / 0.7340\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9738 / 0.7397\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9724 / 0.7410\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9659 / 0.7385\n",
      "Train/Test sample sizes: 1266  /  317\n",
      "=> Final Train/Test:  0.9656 / 0.7423\n"
     ]
    }
   ],
   "source": [
    "del training[:]\n",
    "del testing[:]\n",
    "\n",
    "for i in range(1, nb_features, 10):\n",
    "    tr, te = executingRandomForest(X, y, Xpd, False, 10, i)\n",
    "    training.append(tr)\n",
    "    testing.append(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to adapt...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can obsrved above, the number of trees created by the randm Froest Classifier is proportional to the overfitting of our model. It can be explained by the fact that more trees implies a biggest complexity of our model and so the model overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By default, the numbers of featurs reached is the square root of the number of column (here it is sqrt(133) =~ 11). With a lowest number, the accurancy of our model on the testing set decreases, so a lowest number of feature reduces the overfitting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
