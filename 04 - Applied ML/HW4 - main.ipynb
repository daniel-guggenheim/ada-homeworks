{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Homework 04 - Applied ML</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation of the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = os.path.join('data','CrowdstormingDataJuly1st.csv') \n",
    "df = pd.read_csv(filename)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of dyads (rows in dataframe): ', len(df))\n",
    "print('Total number of interactions between a referee and a player (nb of games): ', sum(df.games))\n",
    "print('Mean number of games for a dyad: ', np.mean(df['games']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning / setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of rows in dataframe: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing raters</b>: We decided to remove row where the two rates were significantly different or if any of the rates were absent (Nan value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf = df.copy()\n",
    "\n",
    "## Removing null values in raters\n",
    "cleandf = cleandf[cleandf[\"rater1\"].notnull() & cleandf[\"rater2\"].notnull()]\n",
    "\n",
    "## Removing all rows where the difference between the two raters is larger than 0.25\n",
    "cleandf['difference'] = abs(cleandf.rater1 - cleandf.rater2)\n",
    "cleandf = cleandf[cleandf['difference'] <= 0.25]\n",
    "cleandf.drop('difference', axis =1, inplace=True)\n",
    "\n",
    "print(\"Number of rows in the cleaned dataframe: \", len(cleandf))\n",
    "print(\"Number of rows removed: \", (len(df)-len(cleandf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Skin tone</b>: Then we decide to take the skin tone as the mean between the two raters. This is the value that will be predicted later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleandf[\"meanSkinTone\"] = abs(cleandf[\"rater1\"] + cleandf[\"rater2\"] ) / 2\n",
    "cleandf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Birthday date</b>: As the classifier can not understand date, we decided to change birthday date in seconds. It seemed important for us to keep the birthday date, as it could help predict the color skin if there were more people from a certain demography that played during some years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_to_seconds(t):\n",
    "    seconds = (pd.to_datetime(t) - datetime.datetime(1970, 1,1)).total_seconds()\n",
    "    return int(seconds)\n",
    "\n",
    "cleandf.birthday = cleandf.birthday.apply(time_to_seconds)\n",
    "cleandf.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Changing data attributes to numerals function</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Dummy variables</i></b>: We noticed that a lot of the columns could not be used in the Random forest as they are non-numerical. As most of these features can be seen as categorical variables, we decided to make dummy variables with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The number of different positions is\", cleandf[\"position\"].unique().size)\n",
    "print(\"The number of different clubs is\",cleandf[\"club\"].unique().size)\n",
    "print(\"The number of different league countries is\",cleandf[\"leagueCountry\"].unique().size)\n",
    "print(\"The number of different referee countries is\",cleandf[\"Alpha_3\"].unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of these datas, we decided to remove the \"referee country\" (Alpha_3) and to make dummy variables with the 3 other categories. We decided to remove the referee country because there were a lot of them and it seemed it would induce more error and overfitting to our classifier than it would help it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Replacing Nan</i></b>: We also realize that the dataframe still have some NaN values. We decided to substitute every NaN with the mean of their column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changeDfAttributesToNumerals(cleandf):\n",
    "    ## Making the dummy variables\n",
    "    dummydf = pd.get_dummies(cleandf, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"position\"], sparse=False, drop_first=False)\n",
    "    dummydf = pd.get_dummies(dummydf, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"club\"], sparse=False, drop_first=False)\n",
    "    dummydf = pd.get_dummies(dummydf, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"leagueCountry\"], sparse=False, drop_first=False)\n",
    "    dummydf.drop(\"Alpha_3\", axis = 1, inplace = True)\n",
    "    \n",
    "    ## Replacing NaN values\n",
    "    # For each column, if there is any NaN value, we compute the mean and replace the NaN values with it.\n",
    "    for i in range(len(dummydf.columns)):\n",
    "        if (dummydf[dummydf.columns[i]].isnull().values.any()):\n",
    "            mean = np.mean(dummydf[dummydf.columns[i]])\n",
    "            dummydf[dummydf.columns[i]].fillna(mean, inplace = True)\n",
    "    \n",
    "    return dummydf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now apply the function to our dataset and observe the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummydf = changeDfAttributesToNumerals(cleandf)\n",
    "print(\"Number of columns in the previous dataframe: \",cleandf.columns.size)\n",
    "print(\"Number of columns in the new dataframe with dummy variables: \",dummydf.columns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dummydf.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing useless columns</b>: For the classifier, there are some columns that it makes no sense to use. These columns are the player name (and short name), the photo and the initial ratings (of rater 1 and 2). Therefore we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usedf = dummydf.drop(['playerShort', 'player', 'photoID', 'rater1', 'rater2']  , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "usedf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will show how we classify the data using random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def executingRandomForest(X, y, Xpd, printingInfo, numb_trees, nb_features):\n",
    "    # Creating kfolds\n",
    "    once = True\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    sumAccurancy_tr = 0\n",
    "    sumAccurancy_te = 0\n",
    "\n",
    "    #Iterating for kfold validation\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        if(once):\n",
    "            print(\"Train/Test sample sizes:\", train_index.size, \" / \", test_index.size)\n",
    "\n",
    "        ## Making indices\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #Make classifier\n",
    "        rf = RandomForestClassifier(n_estimators = numb_trees, max_features = nb_features, n_jobs = 4)\n",
    "\n",
    "        #Train the classifier\n",
    "        clf = rf.fit(X_train, y_train)\n",
    "\n",
    "        # Make prediction for testing data with the classifier\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        #Computing error\n",
    "        rf_train_score = metrics.accuracy_score(y_train, clf.predict(X_train))\n",
    "        rf_test_score = metrics.accuracy_score(y_test, y_pred)\n",
    "        if(printingInfo):\n",
    "            print('Train/Test:  {:.4f} / {:.4f}'.format(rf_train_score, rf_test_score))\n",
    "#             print('   {:.4f}'.format(rf_test_score))\n",
    "        \n",
    "        sumAccurancy_tr = sumAccurancy_tr + rf_train_score\n",
    "        sumAccurancy_te = sumAccurancy_te + rf_test_score\n",
    "\n",
    "\n",
    "        # Features importance score\n",
    "        if(once):\n",
    "            once = False\n",
    "            importances = rf.feature_importances_\n",
    "            std = np.std([tree.feature_importances_ for tree in rf.estimators_],\n",
    "                         axis=0)\n",
    "            indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    #Printing the final result score:\n",
    "    print('=> Final Train/Test:  {:.4f} / {:.4f}'.format(sumAccurancy_tr / 5.0, sumAccurancy_te / 5.0))\n",
    "    \n",
    "    if(printingInfo):\n",
    "        # Plot the feature importances of the forest\n",
    "        fig = plt.figure()\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(X.shape[1]), importances[indices],\n",
    "                   color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        plt.xticks(range(X.shape[1]), importances[indices])\n",
    "        plt.xlim([-1, X.shape[1]])\n",
    "        plt.show()\n",
    "\n",
    "        # Print the feature ranking\n",
    "        print(\"Feature ranking (first 20):\")\n",
    "        for f in range(20):\n",
    "            print(\"%d. feature %s (%f)\" % (f + 1, Xpd.columns[f], importances[indices[f]]))\n",
    "        \n",
    "    # Returns the average accurancy for the testing set and the training set\n",
    "    return (sumAccurancy_tr / 5.0, sumAccurancy_te / 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making X (the features to use in the classifier) and Y (the value to predict). Executing the random forest algorithm on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMatrixForClassifier(df):\n",
    "    y = np.asarray(df[\"meanSkinTone\"].values, dtype=\"|S6\")\n",
    "    Xpd = df.drop(\"meanSkinTone\", axis = 1)\n",
    "    X = Xpd.as_matrix()\n",
    "    return (X, y, Xpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numb_trees = 10\n",
    "nb_features = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X, y, Xpd) = makeMatrixForClassifier(usedf)\n",
    "executingRandomForest(X, y, Xpd, True, numb_trees, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that we have unrealistic results. Ploting the features importance show...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betterResultMaybedf = usedf.drop(['birthday', 'height', 'weight']  , axis=1)\n",
    "(X, y, Xpd) = makeMatrixForClassifier(betterResultMaybedf)\n",
    "executingRandomForest(X, y, Xpd, True, numb_trees, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We see that it leads still to a problem. It is in fact the case that we use the same player in both dataset. Therefore the random forest will train by identifying the player, and as he will always have the same skin color, it will help to predict that. We want to avoid that, therefore we will need to group the players together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Aggregate the referee info by socker player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_columns = ['games', 'victories', 'ties', 'defeats', 'goals', 'yellowCards', 'yellowReds', 'redCards']\n",
    "const_columns = ['playerShort', 'player', 'birthday', 'height', 'weight', 'meanSkinTone']\n",
    "majority_vote = ['club', 'leagueCountry', 'position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "remove_columns = ['photoID', 'refNum', 'refCountry', 'Alpha_3', 'rater1', 'rater2']\n",
    "referee_info_df = cleandf.drop(remove_columns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_group_player = list(referee_info_df.groupby('playerShort'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum_func = lambda x, y: x+y\n",
    "def accumulate(series):\n",
    "    return reduce(sum_func, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_player_data = []\n",
    "for player_name, data in by_group_player:\n",
    "    one_occurrence = {column : data[column].tolist()[0] for column in const_columns}\n",
    "    accumulated = {column : accumulate(data[column].tolist()) for column in acc_columns}\n",
    "    #TODO: comment on: most_common()[0][0] - first 0 stands for max voted value ('name': count); \n",
    "    #                                        second 0 - gives a name from tuple \n",
    "    majority_vote = { column : Counter(data[column].tolist()).most_common()[0][0] for column in majority_vote}\n",
    "\n",
    "    #TODO: add comment\n",
    "    #################################################################################\n",
    "    acc_niat = accumulate(data['nIAT'].tolist())\n",
    "    acc_prod_iat = accumulate((data['meanIAT']*data['nIAT']).tolist())\n",
    "    \n",
    "    acc_nexp = accumulate(data['nExp'].tolist())\n",
    "    acc_prod_exp = accumulate((data['meanExp']*data['nExp']).tolist())\n",
    "    \n",
    "    #TODO: add comment\n",
    "    #################################################################################\n",
    "    acc_se_iat =accumulate((data['nIAT']).tolist())\n",
    "    acc_prod_se_iat = accumulate((data['seIAT']*data['seIAT']*data['nIAT']).tolist())\n",
    "    \n",
    "    acc_se_exp =accumulate((data['nExp']).tolist())\n",
    "    acc_prod_se_exp = accumulate((data['seExp']*data['seExp']*data['nExp']).tolist())\n",
    "    \n",
    "    wm = {'weighted_mean_iat' : acc_prod_iat/acc_niat,\n",
    "          'weighted_mean_exp' : acc_prod_exp/acc_nexp,\n",
    "          'sqrt_weighted_mean_iat' : math.sqrt(acc_prod_se_iat/acc_se_iat),\n",
    "          'sqrt_weighted_mean_exp' : math.sqrt(acc_prod_se_iat/acc_se_iat)}\n",
    "    \n",
    "    unique_player_data.append(\n",
    "        dict(list(one_occurrence.items()) +\n",
    "             list(accumulated.items()) +\n",
    "             list(majority_vote.items()) +\n",
    "             list(wm.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aggregated_df = pd.DataFrame(unique_player_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated_df = pd.get_dummies(aggregated_df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"position\"], sparse=False, drop_first=False)\n",
    "aggregated_df = pd.get_dummies(aggregated_df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"club\"], sparse=False, drop_first=False)\n",
    "aggregated_df = pd.get_dummies(aggregated_df, prefix=None, prefix_sep='_', dummy_na=False, columns=[\"leagueCountry\"], sparse=False, drop_first=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aggregated_df.isnull().values.any()\n",
    "randomForestDF = aggregated_df.drop(['player', 'playerShort'], axis = 1)\n",
    "randomForestDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each column, if there is any NaN value, we compute the mean and replace the NaN values with it.\n",
    "for i in range(len(randomForestDF.columns)):\n",
    "    if (randomForestDF[randomForestDF.columns[i]].isnull().values.any()):\n",
    "        mean = np.mean(randomForestDF[randomForestDF.columns[i]])\n",
    "        randomForestDF[randomForestDF.columns[i]].fillna(mean, inplace = True)\n",
    "        \n",
    "randomForestDF.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(X, y, Xpd) = makeMatrixForClassifier(randomForestDF)\n",
    "executingRandomForest(X, y, Xpd, False, numb_trees, nb_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of features effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Very long to run, BE CAREFUL    \n",
    "training = []\n",
    "testing = []\n",
    "for i in range (1, 100, 10):\n",
    "    print(\"i :\", i)\n",
    "    nb_features = i\n",
    "\n",
    "    tr, te = executingRandomForest(X, y, Xpd, False, numb_trees, nb_features)\n",
    "    training.append(tr)\n",
    "    testing.append(te)\n",
    "\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to adapt...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can obsrved above, the number of trees created by the randm Froest Classifier is proportional to the overfitting of our model. It can be explained by the fact that more trees implies a biggest complexity of our model and so the model overfits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "By default, the numbers of featurs reached is the square root of the number of column (here it is sqrt(133) =~ 11). With a lowest number, the accurancy of our model on the testing set decreases, so a lowest number of feature reduces the overfitting."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [ADA]",
   "language": "python",
   "name": "Python [ADA]"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
